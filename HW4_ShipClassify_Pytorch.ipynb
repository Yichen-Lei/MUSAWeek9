{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"HW4_ShipClassify_Pytorch.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"Lj222Fow4V3P","colab_type":"code","colab":{}},"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load in \n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# Any results you write to the current directory are saved as output."],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kWS2vvgHFBIU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"outputId":"3c56f2eb-13df-4378-dd11-b369c3aaae1e","executionInfo":{"status":"ok","timestamp":1585498582479,"user_tz":240,"elapsed":19978,"user":{"displayName":"Guray Erus","photoUrl":"","userId":"15374812584437350386"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"pBZf18BJ4V3U","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","import os\n","from tqdm import tqdm\n","\n","import matplotlib.image as mpimg\n","import json\n","\n","from skimage.color import rgb2gray\n","from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.decomposition import PCA\n","\n","\n","import matplotlib.pyplot as plt\n","\n","\n","dataFile = '/content/drive/My Drive/PrjData/Prj_Shipsnet/shipsnet.json'\n","\n","with open(dataFile) as data_file:\n","    data = json.load(data_file)\n","labelDf = pd.DataFrame(data)\n","\n","\n","#imgFiles = os.listdir(dataFolder)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PM6lSzzyFARe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"mKL-KAXX4V3Z","colab_type":"code","colab":{}},"source":["import torch\n","import time\n","import copy\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as func\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms, utils\n","from torchvision.transforms import ToTensor,Resize\n","\n","from torchvision import models\n","\n","from torch.optim import lr_scheduler\n","\n","from PIL import Image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"mrO2N4of4V3d","colab_type":"code","colab":{}},"source":["#Defining the data augmentation steps\n","transform = transforms.Compose([\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"H_niSIYf4V3g","colab_type":"code","colab":{}},"source":["#Defining the PyTorch Dataset for retrieving the ships data\n","class ShipDataset(torch.utils.data.Dataset):\n","  def __init__(self, imgFiles, transform, index=None):\n","    \n","    \n","    self.imgFiles = imgFiles\n","    self.transform = transform\n","\n","    \n","    \n","  def __len__(self):\n","    \n","    return len(self.imgFiles)\n","    \n","  def __getitem__(self, index):\n","    \n","    filename = self.imgFiles[index]\n","    \n","    img =  Image.open(os.path.join(dataFolder, filename)).convert('RGB')\n","    \n","    \n","    label = int(filename.split(\"_\")[0])\n","    \n","    \n","    if self.transform is not None:\n","        img = self.transform(img)\n","        \n","    img = img/255.0\n","      \n","#     label = torch.FloatTensor([label])\n","\n","    return img, label"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"-Tk0p5iC4V3j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"a769f749-afb9-4d5d-9966-e3fe7aeaee0c","executionInfo":{"status":"error","timestamp":1585498831996,"user_tz":240,"elapsed":357,"user":{"displayName":"Guray Erus","photoUrl":"","userId":"15374812584437350386"}}},"source":["device='cuda'\n","\n","#Splitting data for train/validation\n","trainDataset = ShipDataset(imgFiles[:3500], transform)\n","testDataset = ShipDataset(imgFiles[3500:], transform)\n","\n","#Defining dataloaders\n","trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=30, \n","                                          shuffle=True, num_workers=2)\n","\n","\n","testLoader = torch.utils.data.DataLoader(testDataset, batch_size=40, \n","                                          shuffle=False, num_workers=2)\n","\n","\n","#Define the loss functions\n","criteria = nn.CrossEntropyLoss()\n","\n"],"execution_count":10,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-1900dcad2351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Splitting data for train/validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrainDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShipDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgFiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtestDataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mShipDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgFiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'imgFiles' is not defined"]}]},{"cell_type":"code","metadata":{"trusted":true,"id":"XJt4zRl84V3n","colab_type":"code","colab":{}},"source":["image_datasets = {'train':trainDataset, 'val':testDataset}\n","\n","dataloaders = {\"train\":trainLoader, \"val\":testLoader}\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"_kg_hide-output":true,"id":"4cN4VtN_4V3r","colab_type":"code","colab":{}},"source":["#Defining model architecture. Transfer Learning using Resnet34\n","net = models.resnet34(pretrained=True)\n","net.fc = nn.Linear(512,2)\n","\n","\n","net.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","#Defining Learning Rate scheduler\n","exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"khfXQ9PX4V3u","colab_type":"code","colab":{}},"source":["def freezeLayers(net):\n","    \n","    ## Freeze all layers\n","    for child in net.children():\n","        for param in child.parameters():\n","            param.requires_grad = False\n","\n","    ## Unfreezing the last FC layer        \n","    for param in list(net.children())[-1].parameters():\n","        param.requires_grad = True\n","        \n","    return net\n","    \n","    \n","def unfreezeLayers(net):\n","    \n","    ## Freeze all layers\n","    for child in net.children():\n","        for param in child.parameters():\n","            param.requires_grad = True\n","            \n","    return net"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"spXit_RQ4V3x","colab_type":"code","colab":{}},"source":["def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","            if phase == 'train':\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","    return model, epoch_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"gtg4gVbr4V30","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"2pGM3UyR4V35","colab_type":"code","colab":{}},"source":["#Freezing the backbone and training\n","net = freezeLayers(net)\n","optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), 1e-3)\n","\n","train_model(net,criterion, optimizer, exp_lr_scheduler, num_epochs = 6)\n","\n","#Training the whole network\n","net = unfreezeLayers(net)\n","optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), 1e-4)\n","\n","train_model(net,criterion, optimizer, exp_lr_scheduler, num_epochs = 6)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"3ZNd_nNt4V38","colab_type":"code","colab":{}},"source":["def runTest(net, criterion, optimizer,exp_lr_scheduler ):\n","    \n","    net = freezeLayers(net)\n","    model, acc = train_model(net,criterion, optimizer, exp_lr_scheduler, num_epochs = 6)\n","    \n","    net = unfreezeLayers(net)\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), 1e-4)\n","\n","    model, acc = train_model(net,criterion, optimizer, exp_lr_scheduler, num_epochs = 6)\n","    \n","    print(f\"Best Accuracy : {acc}\")\n","    \n","    return acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"TrTF0Q774V3_","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"fLYMaK7V4V4C","colab_type":"code","colab":{}},"source":["#Performing 10-Fold Cross Validation\n","imgFiles = np.array(imgFiles)\n","\n","accList = []\n","\n","\n","for i in tqdm(range(10)):\n","    \n","    test = list(range(i*setSize, (i+1)*setSize))\n","    train = list(set(list(range(4000))).difference(set(test)))\n","    \n","    trainDataset = ShipDataset(imgFiles[train], transform)\n","    testDataset = ShipDataset(imgFiles[test], transform)\n","\n","\n","    trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size=30, \n","                                              shuffle=True, num_workers=2)\n","\n","\n","    testLoader = torch.utils.data.DataLoader(testDataset, batch_size=40, \n","                                              shuffle=False, num_workers=2)\n","    \n","    \n","    image_datasets = {'train':trainDataset, 'val':testDataset}\n","    dataloaders = {\"train\":trainLoader, \"val\":testLoader}\n","    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","    \n","    \n","    net = models.resnet34(pretrained=True)\n","    net.fc = nn.Linear(512,2)\n","    net.to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), 1e-3)\n","    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)\n","    \n","    \n","    acc = runTest(net, criterion, optimizer,exp_lr_scheduler)\n","    accList.append(acc)\n","\n","    \n","    \n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"2eAfuhAA4V4F","colab_type":"code","colab":{}},"source":["accList"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"YFDyfbhW4V4H","colab_type":"code","colab":{}},"source":["np.mean([x.cpu().numpy() for x in accList])"],"execution_count":0,"outputs":[]}]}